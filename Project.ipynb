{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVG-ZXFmh33B"
      },
      "source": [
        "Hidden Markov Model for POS Tagging.\n",
        "\n",
        "This program trains on a POS-tagged corpus and then tag the untagged corpus and then display the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTRtWf8OCMLk",
        "outputId": "fa0fd8d0-7dec-4fc7-abb2-957504fe3a74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (4.5.3)\n",
            "Sentence: ['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
            "Tags: ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n",
            "\n",
            "Sentence: ['[', 'This', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']\n",
            "Tags: ['PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'AUX', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADP', 'NOUN', 'PART', 'VERB', 'PUNCT', 'PUNCT']\n",
            "\n",
            "Sentence: ['DPA', ':', 'Iraqi', 'authorities', 'announced', 'that', 'they', 'had', 'busted', 'up', '3', 'terrorist', 'cells', 'operating', 'in', 'Baghdad', '.']\n",
            "Tags: ['PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'SCONJ', 'PRON', 'AUX', 'VERB', 'ADP', 'NUM', 'ADJ', 'NOUN', 'VERB', 'ADP', 'PROPN', 'PUNCT']\n",
            "\n",
            "Sentence: ['Two', 'of', 'them', 'were', 'being', 'run', 'by', '2', 'officials', 'of', 'the', 'Ministry', 'of', 'the', 'Interior', '!']\n",
            "Tags: ['NUM', 'ADP', 'PRON', 'AUX', 'AUX', 'VERB', 'ADP', 'NUM', 'NOUN', 'ADP', 'DET', 'PROPN', 'ADP', 'DET', 'PROPN', 'PUNCT']\n",
            "\n",
            "Sentence: ['The', 'MoI', 'in', 'Iraq', 'is', 'equivalent', 'to', 'the', 'US', 'FBI', ',', 'so', 'this', 'would', 'be', 'like', 'having', 'J.', 'Edgar', 'Hoover', 'unwittingly', 'employ', 'at', 'a', 'high', 'level', 'members', 'of', 'the', 'Weathermen', 'bombers', 'back', 'in', 'the', '1960s', '.']\n",
            "Tags: ['DET', 'PROPN', 'ADP', 'PROPN', 'AUX', 'ADJ', 'ADP', 'DET', 'PROPN', 'PROPN', 'PUNCT', 'ADV', 'PRON', 'AUX', 'AUX', 'SCONJ', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'ADV', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'PROPN', 'NOUN', 'ADV', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import conllu\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Read training data\n",
        "with open('en_ewt-ud-train.conllu', 'rb') as f:\n",
        "    train_data_str = f.read().decode('utf-8')\n",
        "train_data = conllu.parse(train_data_str)\n",
        "\n",
        "# Read test data\n",
        "with open('en_ewt-ud-test.conllu', 'rb') as f:\n",
        "    test_data_str = f.read().decode('utf-8')\n",
        "test_data = conllu.parse(test_data_str)\n",
        "\n",
        "# Extract words and POS tags from training data\n",
        "train_words = []\n",
        "train_tags = []\n",
        "for sentence in train_data:\n",
        "    for token in sentence:\n",
        "        train_words.append(token['form'])\n",
        "        train_tags.append(token['upostag'])\n",
        "\n",
        "#Print 5 sentences in train_data\n",
        "for i, sentence in enumerate(train_data):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    sentence_words = [token['form'] for token in sentence]\n",
        "    sentence_tags = [token['upostag'] for token in sentence]\n",
        "    print('Sentence:', sentence_words)\n",
        "    print('Tags:', sentence_tags)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZxZLsOug2HV",
        "outputId": "c2261366-066e-4b67-9430-2a6d12b8cfee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Word | POS Tag |\n",
            "| --- | --- |\n",
            "| Aerocom | NOUN |\n",
            "| anti-American | X |\n",
            "| listeners | PUNCT |\n",
            "| tick | INTJ |\n",
            "| Bart | SCONJ |\n",
            "| Bowtie | PRON |\n",
            "| dropped | NUM |\n",
            "| thursday | SYM |\n",
            "| melted | AUX |\n",
            "| deeper | _ |\n",
            "| terribly | PART |\n",
            "| Muhammad | ADJ |\n",
            "| Global's | PROPN |\n",
            "| morbidity | ADP |\n",
            "| made | ADV |\n",
            "| Shakespearean | CCONJ |\n",
            "| intrigue | DET |\n",
            "| Beardies | VERB |\n"
          ]
        }
      ],
      "source": [
        "# Get unique words and tags\n",
        "unique_words = list(set(train_words))\n",
        "unique_tags = list(set(train_tags))\n",
        "\n",
        "# Create word and tag dictionaries for mapping\n",
        "word2idx = {word: idx for idx, word in enumerate(unique_words)}\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "\n",
        "# Print unique words and tags\n",
        "print(\"| Word | POS Tag |\")\n",
        "print(\"| --- | --- |\")\n",
        "for word, tag in zip(unique_words, unique_tags):\n",
        "    print(f\"| {word} | {tag} |\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HKGOqCnDhtvV"
      },
      "outputs": [],
      "source": [
        "# Calculate transition and emission probabilities\n",
        "N_STATES = len(unique_tags)\n",
        "N_OBSERVATIONS = len(unique_words)\n",
        "\n",
        "# Initialize transition and emission probability matrices to zeros\n",
        "transition_probs = np.zeros((N_STATES, N_STATES))\n",
        "emission_probs = np.zeros((N_STATES, N_OBSERVATIONS))\n",
        "\n",
        "# Loop through the training data and calculate transition and emission probabilities\n",
        "prev_tag_idx = None\n",
        "for word, tag in zip(train_words, train_tags):\n",
        "    # Map word and tag to their corresponding integer indices\n",
        "    word_idx = word2idx[word]\n",
        "    tag_idx = tag2idx[tag]\n",
        "\n",
        "    # Update transition probability from previous tag to current tag\n",
        "    if prev_tag_idx is not None:\n",
        "        transition_probs[prev_tag_idx, tag_idx] += 1\n",
        "\n",
        "    # Update emission probability of current tag for current word\n",
        "    emission_probs[tag_idx, word_idx] += 1\n",
        "\n",
        "    # Update previous tag index\n",
        "    prev_tag_idx = tag_idx\n",
        "\n",
        "# Normalize transition and emission probabilities\n",
        "transition_probs /= transition_probs.sum(axis=1, keepdims=True)\n",
        "emission_probs /= emission_probs.sum(axis=1, keepdims=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2hiC-SduGwV"
      },
      "source": [
        "**visualize the steps for calculating the transition and emission probabilities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "yszlgpgStsU4",
        "outputId": "fcd0ff01-be2d-4829-e139-4e1f67cbec42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": [
              "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
              "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
              " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
              "<!-- Generated by graphviz version 2.43.0 (0)\n",
              " -->\n",
              "<!-- Title: %3 Pages: 1 -->\n",
              "<svg width=\"508pt\" height=\"548pt\"\n",
              " viewBox=\"0.00 0.00 508.30 548.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
              "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 544)\">\n",
              "<title>%3</title>\n",
              "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-544 504.3,-544 504.3,4 -4,4\"/>\n",
              "<!-- Start -->\n",
              "<g id=\"node1\" class=\"node\">\n",
              "<title>Start</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"223.06\" cy=\"-522\" rx=\"27.1\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"223.06\" y=\"-518.3\" font-family=\"Times,serif\" font-size=\"14.00\">Start</text>\n",
              "</g>\n",
              "<!-- N_STATES -->\n",
              "<g id=\"node2\" class=\"node\">\n",
              "<title>N_STATES</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"223.06\" cy=\"-450\" rx=\"121.58\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"223.06\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">N_STATES = len(unique_tags)</text>\n",
              "</g>\n",
              "<!-- Start&#45;&gt;N_STATES -->\n",
              "<g id=\"edge1\" class=\"edge\">\n",
              "<title>Start&#45;&gt;N_STATES</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M223.06,-503.7C223.06,-495.98 223.06,-486.71 223.06,-478.11\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"226.56,-478.1 223.06,-468.1 219.56,-478.1 226.56,-478.1\"/>\n",
              "</g>\n",
              "<!-- N_OBSERVATIONS -->\n",
              "<g id=\"node3\" class=\"node\">\n",
              "<title>N_OBSERVATIONS</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"223.06\" cy=\"-378\" rx=\"162.47\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"223.06\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">N_OBSERVATIONS = len(unique_words)</text>\n",
              "</g>\n",
              "<!-- N_STATES&#45;&gt;N_OBSERVATIONS -->\n",
              "<g id=\"edge2\" class=\"edge\">\n",
              "<title>N_STATES&#45;&gt;N_OBSERVATIONS</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M223.06,-431.7C223.06,-423.98 223.06,-414.71 223.06,-406.11\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"226.56,-406.1 223.06,-396.1 219.56,-406.1 226.56,-406.1\"/>\n",
              "</g>\n",
              "<!-- transition_probs_init -->\n",
              "<g id=\"node4\" class=\"node\">\n",
              "<title>transition_probs_init</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"223.06\" cy=\"-306\" rx=\"131.88\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"223.06\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">Initialize transition_probs to zeros</text>\n",
              "</g>\n",
              "<!-- N_OBSERVATIONS&#45;&gt;transition_probs_init -->\n",
              "<g id=\"edge3\" class=\"edge\">\n",
              "<title>N_OBSERVATIONS&#45;&gt;transition_probs_init</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M223.06,-359.7C223.06,-351.98 223.06,-342.71 223.06,-334.11\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"226.56,-334.1 223.06,-324.1 219.56,-334.1 226.56,-334.1\"/>\n",
              "</g>\n",
              "<!-- emission_probs_init -->\n",
              "<g id=\"node5\" class=\"node\">\n",
              "<title>emission_probs_init</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"223.06\" cy=\"-234\" rx=\"129.98\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"223.06\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">Initialize emission_probs to zeros</text>\n",
              "</g>\n",
              "<!-- transition_probs_init&#45;&gt;emission_probs_init -->\n",
              "<g id=\"edge4\" class=\"edge\">\n",
              "<title>transition_probs_init&#45;&gt;emission_probs_init</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M223.06,-287.7C223.06,-279.98 223.06,-270.71 223.06,-262.11\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"226.56,-262.1 223.06,-252.1 219.56,-262.1 226.56,-262.1\"/>\n",
              "</g>\n",
              "<!-- Loop -->\n",
              "<g id=\"node6\" class=\"node\">\n",
              "<title>Loop</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"223.06\" cy=\"-162\" rx=\"105.88\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"223.06\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">Loop through training data</text>\n",
              "</g>\n",
              "<!-- emission_probs_init&#45;&gt;Loop -->\n",
              "<g id=\"edge5\" class=\"edge\">\n",
              "<title>emission_probs_init&#45;&gt;Loop</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M223.06,-215.7C223.06,-207.98 223.06,-198.71 223.06,-190.11\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"226.56,-190.1 223.06,-180.1 219.56,-190.1 226.56,-190.1\"/>\n",
              "</g>\n",
              "<!-- Map -->\n",
              "<g id=\"node7\" class=\"node\">\n",
              "<title>Map</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"336.06\" cy=\"-90\" rx=\"112.38\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"336.06\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Map word and tag to indices</text>\n",
              "</g>\n",
              "<!-- Loop&#45;&gt;Map -->\n",
              "<g id=\"edge6\" class=\"edge\">\n",
              "<title>Loop&#45;&gt;Map</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M249.84,-144.41C264.92,-135.06 283.97,-123.27 300.31,-113.14\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"302.42,-115.95 309.08,-107.71 298.73,-110 302.42,-115.95\"/>\n",
              "</g>\n",
              "<!-- Normalize -->\n",
              "<g id=\"node10\" class=\"node\">\n",
              "<title>Normalize</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"111.06\" cy=\"-90\" rx=\"94.78\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"111.06\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">Normalize probabilities</text>\n",
              "</g>\n",
              "<!-- Loop&#45;&gt;Normalize -->\n",
              "<g id=\"edge11\" class=\"edge\">\n",
              "<title>Loop&#45;&gt;Normalize</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M196.51,-144.41C181.45,-135 162.41,-123.1 146.13,-112.92\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"147.74,-109.8 137.41,-107.47 144.03,-115.74 147.74,-109.8\"/>\n",
              "</g>\n",
              "<!-- Transition_Update -->\n",
              "<g id=\"node8\" class=\"node\">\n",
              "<title>Transition_Update</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"128.06\" cy=\"-18\" rx=\"95.58\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"128.06\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Update transition_probs</text>\n",
              "</g>\n",
              "<!-- Map&#45;&gt;Transition_Update -->\n",
              "<g id=\"edge7\" class=\"edge\">\n",
              "<title>Map&#45;&gt;Transition_Update</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M290.15,-73.55C258.36,-62.85 215.89,-48.56 182.41,-37.29\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"183.36,-33.92 172.77,-34.05 181.13,-40.56 183.36,-33.92\"/>\n",
              "</g>\n",
              "<!-- Emission_Update -->\n",
              "<g id=\"node9\" class=\"node\">\n",
              "<title>Emission_Update</title>\n",
              "<ellipse fill=\"none\" stroke=\"black\" cx=\"406.06\" cy=\"-18\" rx=\"94.48\" ry=\"18\"/>\n",
              "<text text-anchor=\"middle\" x=\"406.06\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Update emission_probs</text>\n",
              "</g>\n",
              "<!-- Map&#45;&gt;Emission_Update -->\n",
              "<g id=\"edge8\" class=\"edge\">\n",
              "<title>Map&#45;&gt;Emission_Update</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M353,-72.05C361.69,-63.37 372.41,-52.64 381.94,-43.11\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"384.67,-45.33 389.27,-35.79 379.72,-40.39 384.67,-45.33\"/>\n",
              "</g>\n",
              "<!-- Transition_Update&#45;&gt;Loop -->\n",
              "<g id=\"edge9\" class=\"edge\">\n",
              "<title>Transition_Update&#45;&gt;Loop</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M64.86,-31.64C42.74,-39.43 20.17,-51.94 7.06,-72 -1.7,-85.39 -2.82,-95.41 7.06,-108 22.53,-127.72 77.87,-141.08 128.66,-149.44\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"128.32,-152.93 138.74,-151.05 129.42,-146.02 128.32,-152.93\"/>\n",
              "</g>\n",
              "<!-- Emission_Update&#45;&gt;Loop -->\n",
              "<g id=\"edge10\" class=\"edge\">\n",
              "<title>Emission_Update&#45;&gt;Loop</title>\n",
              "<path fill=\"none\" stroke=\"black\" d=\"M427.38,-35.7C448.58,-54.34 475.51,-84.9 457.06,-108 439.89,-129.49 378.15,-142.91 322.41,-150.87\"/>\n",
              "<polygon fill=\"black\" stroke=\"black\" points=\"321.87,-147.41 312.44,-152.25 322.83,-154.35 321.87,-147.41\"/>\n",
              "</g>\n",
              "</g>\n",
              "</svg>\n"
            ],
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7be2accdbaf0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Install Graphviz library\n",
        "!pip install graphviz\n",
        "\n",
        "# Import necessary libraries\n",
        "from graphviz import Digraph\n",
        "from IPython.display import display\n",
        "\n",
        "# Create a new directed graph\n",
        "dot = Digraph()\n",
        "\n",
        "# Add nodes\n",
        "dot.node('Start', 'Start')\n",
        "dot.node('N_STATES', 'N_STATES = len(unique_tags)')\n",
        "dot.node('N_OBSERVATIONS', 'N_OBSERVATIONS = len(unique_words)')\n",
        "dot.node('transition_probs_init', 'Initialize transition_probs to zeros')\n",
        "dot.node('emission_probs_init', 'Initialize emission_probs to zeros')\n",
        "dot.node('Loop', 'Loop through training data')\n",
        "dot.node('Map', 'Map word and tag to indices')\n",
        "dot.node('Transition_Update', 'Update transition_probs')\n",
        "dot.node('Emission_Update', 'Update emission_probs')\n",
        "dot.node('Normalize', 'Normalize probabilities')\n",
        "\n",
        "# Add edges to show the flow\n",
        "dot.edge('Start', 'N_STATES')\n",
        "dot.edge('N_STATES', 'N_OBSERVATIONS')\n",
        "dot.edge('N_OBSERVATIONS', 'transition_probs_init')\n",
        "dot.edge('transition_probs_init', 'emission_probs_init')\n",
        "dot.edge('emission_probs_init', 'Loop')\n",
        "dot.edge('Loop', 'Map')\n",
        "dot.edge('Map', 'Transition_Update')\n",
        "dot.edge('Map', 'Emission_Update')\n",
        "dot.edge('Transition_Update', 'Loop')\n",
        "dot.edge('Emission_Update', 'Loop')\n",
        "dot.edge('Loop', 'Normalize')\n",
        "\n",
        "# Display the graph inline\n",
        "display(dot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CMJDINXPuVqF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a smoothing parameter\n",
        "smoothing = 0.0001\n",
        "\n",
        "# Calculate emission probabilities with smoothing\n",
        "emission_probs_smoothed = (emission_probs + smoothing) / (emission_probs.sum(axis=1, keepdims=True) + smoothing * N_OBSERVATIONS)\n",
        "\n",
        "\n",
        "# Define a special token for unknown words\n",
        "UNK_TOKEN = '<UNK>'\n",
        "\n",
        "# Create word and tag dictionaries for mapping, including the special token\n",
        "word2idx = {word: idx for idx, word in enumerate(unique_words + [UNK_TOKEN])}\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(unique_tags + [UNK_TOKEN])}\n",
        "\n",
        "# Calculate emission probabilities with smoothing, including the special token\n",
        "emission_probs_smoothed = np.zeros((N_STATES, N_OBSERVATIONS + 1))\n",
        "for tag_idx in range(N_STATES):\n",
        "    for word_idx in range(N_OBSERVATIONS):\n",
        "        emission_probs_smoothed[tag_idx, word_idx] = (emission_probs[tag_idx, word_idx] + smoothing) / (emission_probs[tag_idx].sum() + smoothing * N_OBSERVATIONS)\n",
        "    # Assign the smoothed probability for the special token\n",
        "    emission_probs_smoothed[tag_idx, N_OBSERVATIONS] = smoothing / (emission_probs[tag_idx].sum() + smoothing * N_OBSERVATIONS)\n",
        "\n",
        "# Function to get word index with handling of unknown words\n",
        "def get_word_index(word):\n",
        "    return word2idx.get(word, N_OBSERVATIONS)  # Return N_OBSERVATIONS for unknown words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKAjXYtmv1nU",
        "outputId": "630156bf-bf0e-48d1-9506-5edf52b58fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1:\n",
            "Original: What if Google Morphed Into GoogleOS ?\n",
            "Replaced: What if Google <UNK> Into <UNK> ?\n",
            "\n",
            "Sentence 2:\n",
            "Original: What if Google expanded on its search - engine ( and now e-mail ) wares into a full - fledged operating system ?\n",
            "Replaced: What if Google expanded on its search - engine ( and now e-mail ) <UNK> into a full - <UNK> operating system ?\n",
            "\n",
            "Sentence 3:\n",
            "Original: [ via Microsoft Watch from Mary Jo Foley ]\n",
            "Replaced: [ via Microsoft Watch from Mary <UNK> <UNK> ]\n",
            "\n",
            "Sentence 4:\n",
            "Original: ( And , by the way , is anybody else just a little nostalgic for the days when that was a good thing ? )\n",
            "Replaced: ( And , by the way , is anybody else just a little <UNK> for the days when that was a good thing ? )\n",
            "\n",
            "Sentence 5:\n",
            "Original: This BuzzMachine post argues that Google's Google 's rush toward ubiquity might backfire -- which we've we 've all heard before , but it's it 's particularly well - put in this post .\n",
            "Replaced: This <UNK> post <UNK> that <UNK> Google 's rush toward <UNK> might <UNK> -- which we've we 've all heard before , but it's it 's particularly well - put in this post .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Process the test data to replace unknown words with '<UNK>'\n",
        "def replace_unknown_words(sentence, word2idx, unk_token):\n",
        "    return [word if word in word2idx else unk_token for word in sentence]\n",
        "\n",
        "# Find and display sentences with unknown words\n",
        "def find_sentences_with_unk(test_data, word2idx, unk_token):\n",
        "    sentences_with_unk = []\n",
        "    for sentence in test_data:\n",
        "        sentence_words = [token['form'] for token in sentence]\n",
        "        replaced_sentence = replace_unknown_words(sentence_words, word2idx, unk_token)\n",
        "        if unk_token in replaced_sentence:\n",
        "            sentences_with_unk.append((sentence_words, replaced_sentence))\n",
        "    return sentences_with_unk\n",
        "\n",
        "# Get sentences with unknown words\n",
        "unk_sentences = find_sentences_with_unk(test_data, word2idx, UNK_TOKEN)\n",
        "\n",
        "# Display examples\n",
        "for i, (original_sentence, replaced_sentence) in enumerate(unk_sentences[:5]):\n",
        "    print(f\"Sentence {i+1}:\")\n",
        "    print(\"Original:\", \" \".join(original_sentence))\n",
        "    print(\"Replaced:\", \" \".join(replaced_sentence))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEL8nBFJ1YAl"
      },
      "outputs": [],
      "source": [
        "# Implement Viterbi algorithm with handling of unknown words\n",
        "def viterbi(obs, states, trans, emiss):\n",
        "    T = len(obs)\n",
        "    N = len(states)\n",
        "    delta = np.zeros((T, N))\n",
        "    delta[0] = emiss[:, get_word_index(obs[0])]\n",
        "    psi = np.zeros((T, N), dtype=int)\n",
        "\n",
        "    for t in range(1, T):\n",
        "        for n in range(N):\n",
        "            word_idx = get_word_index(obs[t])\n",
        "            if word_idx == N_OBSERVATIONS:\n",
        "                # For unknown words, use smoothed emission probabilities\n",
        "                emit_prob = emission_probs_smoothed[n, word_idx]\n",
        "            else:\n",
        "                emit_prob = emiss[n, word_idx]\n",
        "\n",
        "            # Calculate delta with emission probability for unknown words\n",
        "            max_delta = 0\n",
        "            for m in range(N):\n",
        "                temp_delta = delta[t-1, m] * trans[m, n] * emit_prob\n",
        "                if temp_delta > max_delta:\n",
        "                    max_delta = temp_delta\n",
        "                    psi[t, n] = m\n",
        "            delta[t, n] = max_delta\n",
        "\n",
        "    # Backtrack to find the best sequence\n",
        "    best_seq = np.zeros(T, dtype=int)\n",
        "    best_seq[T-1] = np.argmax(delta[T-1])\n",
        "    for t in range(T-2, -1, -1):\n",
        "        best_seq[t] = psi[t+1, best_seq[t+1]]\n",
        "\n",
        "    return best_seq\n",
        "\n",
        "# Evaluate the trained model with handling of unknown words\n",
        "def evaluate_hmm(test_data, word2idx, tag2idx):\n",
        "    predicted_tags = []\n",
        "    true_tags = []\n",
        "\n",
        "    for sentence in test_data:\n",
        "        obs = [token['form'] for token in sentence]\n",
        "        true_tags.extend([tag2idx[token['upostag']] for token in sentence])\n",
        "\n",
        "        # Predict using Viterbi algorithm with handling of unknown words\n",
        "        pred_tags = viterbi(obs, unique_tags, transition_probs, emission_probs_smoothed)\n",
        "        predicted_tags.extend(pred_tags)\n",
        "\n",
        "    return predicted_tags, true_tags\n",
        "\n",
        "# Evaluate the trained model with handling of unknown words\n",
        "predicted_tags, true_tags = evaluate_hmm(test_data, word2idx, tag2idx)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_tags, predicted_tags)\n",
        "print(\"Accuracy with unknown word handling:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C-1NqRviLMN"
      },
      "source": [
        "Conditional Random Field program\n",
        "\n",
        "This below program should have the same function and do the work the same as the previous program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_rbFEC7icZ7"
      },
      "outputs": [],
      "source": [
        "!pip install conllu\n",
        "!pip install sklearn_crfsuite\n",
        "import numpy as np\n",
        "import conllu\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Read training data\n",
        "with open('en_ewt-ud-train.conllu', 'rb') as f:\n",
        "    train_data_str = f.read().decode('utf-8')\n",
        "train_data = conllu.parse(train_data_str)\n",
        "\n",
        "# Read test data\n",
        "with open('en_ewt-ud-test.conllu', 'rb') as f:\n",
        "    test_data_str = f.read().decode('utf-8')\n",
        "test_data = conllu.parse(test_data_str)\n",
        "\n",
        "# Function to extract features from a sentence\n",
        "def extract_features(sentence, idx):\n",
        "    word = sentence[idx]['form']\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_first': idx == 0,\n",
        "        'is_last': idx == len(sentence) - 1,\n",
        "        'prev_word': '' if idx == 0 else sentence[idx - 1]['form'],\n",
        "        'next_word': '' if idx == len(sentence) - 1 else sentence[idx + 1]['form'],\n",
        "        'prev_tag': '' if idx == 0 else sentence[idx - 1]['upostag'],\n",
        "        'next_tag': '' if idx == len(sentence) - 1 else sentence[idx + 1]['upostag']\n",
        "    }\n",
        "    return features\n",
        "\n",
        "# Function to extract features for the entire dataset\n",
        "def extract_dataset_features(dataset):\n",
        "    X = []\n",
        "    y = []\n",
        "    for sentence in dataset:\n",
        "        X_sentence = [extract_features(sentence, i) for i in range(len(sentence))]\n",
        "        y_sentence = [token['upostag'] for token in sentence]\n",
        "        X.append(X_sentence)\n",
        "        y.append(y_sentence)\n",
        "    return X, y\n",
        "\n",
        "# Transform dataset to features and labels\n",
        "X_train, y_train = extract_dataset_features(train_data)\n",
        "X_test, y_test = extract_dataset_features(test_data)\n",
        "\n",
        "# Initialize CRF model\n",
        "crf = CRF()\n",
        "\n",
        "# Train CRF model\n",
        "crf.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels for test data\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "# Flatten true and predicted labels\n",
        "y_true_flat = [tag for sentence_tags in y_test for tag in sentence_tags]\n",
        "y_pred_flat = [tag for sentence_tags in y_pred for tag in sentence_tags]\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true_flat, y_pred_flat))\n",
        "\n",
        "# Display the dependency count for words and tags\n",
        "def display_dependency_count():\n",
        "    num_word_dependencies = 3  # current word, previous word, next word\n",
        "    num_tag_dependencies = 2  # previous tag, next tag\n",
        "\n",
        "    print(f\"The CRF model depends on {num_word_dependencies} words and {num_tag_dependencies} tags to tag the current word.\")\n",
        "\n",
        "# Call the function to display dependency count\n",
        "display_dependency_count()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
